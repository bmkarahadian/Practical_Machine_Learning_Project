---
title: 'Practical Machine Learning: Project'
author: "B. Karahadian"
date: "5/19/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd("/Users/Benjamin/Desktop/ML Class Project/")
```
The purpose of this project is to successfully fit a machine learning algorithm to the HAR (Human Activity Recognition) Weight Lifting dataset. In this dataset, participants were equipped with activity monitors while they performed 5 different movements: perfect dumbbell curl (classe = A) and imperfect dumbbell curls (classe = B, C, D, E). Using the metrics generated by the activity monitors, we will create an algorithm that can successfully predict which motion the participant is conducting.

## Exploration and Cleaning
```{r, message=FALSE}
library(tidyverse)
library(caret)
```

First, let's load the data and take a preliminary look at them.
```{r, warning=FALSE, message = FALSE}
train <- read_csv("pml-training.csv")
test <- read_csv("pml-testing.csv")

train
```

Here we can see the total counts of the different classes throughout the training data:
```{r}
train %>%
    count(classe)
```
As we can see, each class is sufficiently represented to not have to worry about weights when modeling.

The variables "user_name" and "X1", the name of the participant and the observation number, respectively, will not help us with prediction so we can remove them. Neither will the timestamp variables provide any predictive value.
```{r}
train <- 
    train %>%
    select(-X1, -user_name, -raw_timestamp_part_1, -raw_timestamp_part_2, -cvtd_timestamp)
```
As we can see, there are multiple variables that are nearly completely populated with NA's.
```{r}
map_dbl(train, function(x) {
    is.na(x) %>%
        sum()
}) %>%
    head(25)
```
These variables should be removed from our dataset.
```{r}
na_vars <- 
    map_dbl(train, function(x) {
        is.na(x) %>%
            sum()
    }) %>%
        `>`(0) %>%
        which()

train <- 
    train %>%
        select(-all_of(na_vars))
```


And here is our final training dataset before modeling:
```{r, include=FALSE}
train
```


## Modeling
To model these data, we will first generate three separate models: a random forest, a boosted decision tree, and a radial support vector machine. Then we will stack these models to see if we can eke out some more predictive power.
First, we split the training data into three different sections, so we can validate our two tiers of models.
```{r}
set.seed(1111)
inds <- createDataPartition(train$classe, times = 1, p = 0.5)
train2 <- train[inds[[1]],]
out <- train[-inds[[1]],]

set.seed(2222)
inds_out <- createDataPartition(out$classe, times = 1, 0.9)
valid <- out[inds_out[[1]],]
est_test <- out[-inds_out[[1]],]
```

Now, we will train the three individual models on the "train2" data. Note that we use 10-fold cross validation to tune the parameters of the models. A bootstrapping method may yield more highly optimized results, but my local machine is not up to that computational task.

```{r, cache=TRUE}
mod_rf <- 
    train(classe ~ ., data = train2, method = "rf",
          trControl = trainControl(method = "cv", number = 10))

mod_boost <- 
    train(classe ~ ., data = train2, method = "AdaBoost.M1",
          trControl = trainControl(method = "cv", number = 10))

mod_svm <- 
    train(classe ~ ., data = train2, method = "svmRadial",
          trControl = trainControl(method = "cv", number = 10))
```

### Random Forest
Here is the prediction accuracy for our random forest algorithm evaluated on the left-out validation set:
```{r}
valid %>%
    mutate(rf_preds = predict(mod_rf, valid)) %>%
    summarize(mean(rf_preds == classe))
```

It is very accurate. 
As we can see from this diagnostic plot, the algorithm maxes out its accuracy at 28 randomly selected predictors per tree.
```{r}
plot.train(mod_rf)
```


### Boosted Tree
Here is the prediction accuracy for our boosted decision tree evaluated on the left-out validation set:
```{r}
valid %>%
    mutate(boost_preds = predict(mod_boost, valid)) %>%
    summarize(mean(boost_preds == classe))
```

It is also very accurate, but not quite as good the random forest.

As we can see from this diagnotic plot, the algorith maxes out its accuracy using the Zhu method, a maximum tree-depth of 3, and over 140 trees.
```{r}
plot.train(mod_boost)
```

### SVM
Here is the prediction accuracy for our radial support vector machine evaluated on the left-out validation set:
```{r}
valid %>%
    mutate(svm_preds = predict(mod_svm, valid)) %>%
    summarize(mean(svm_preds == classe))
```

As before, this model is very accurate but not quite as good as the two previous.

As we can see from this diagnostic plot, the algorithm maxes out its accuracy with a cost hyperparameter of 1.
```{r}
plot.train(mod_svm)
```

### Stacked Model
Now, we will use the predictions from these three models to train a stacked random forest algorithm to try to beat the predicton accuracy of the single random forest model.

```{r, cache = TRUE}
valid <- 
    valid %>%
        mutate(rf_preds = predict(mod_rf, valid),
               boost_preds = predict(mod_boost, valid),
               svm_preds = predict(mod_svm, valid))

mod_stack <- train(classe ~ rf_preds + boost_preds + svm_preds, data = valid, method = "rf",
                   trControl = trainControl(method = "cv", number = 10))
```

Here is its prediction accuracy:
```{r}
est_test <- 
    est_test %>%
        mutate(rf_preds = predict(mod_rf, est_test),
               boost_preds = predict(mod_boost, est_test),
               svm_preds = predict(mod_svm, est_test))

est_test %>%
    mutate(stack_preds = predict(mod_stack, est_test)) %>%
    summarize(mean(stack_preds == classe))
```

It beats the single random forest model by 0.002. Not much of an improvement, but 0.997 is pretty damn good.
